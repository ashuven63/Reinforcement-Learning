{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from reinforcementLearningCar.nn import neural_net, LossHistory\n",
    "import os.path\n",
    "import timeit\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 19:34:50,066] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space\n",
      "Box(1,)\n",
      "Observation Space\n",
      "Box(3,)\n"
     ]
    }
   ],
   "source": [
    "NUM_INPUT = 2\n",
    "GAMMA = 0.9  # Forgetting.\n",
    "TUNING = False  # If False, just use arbitrary, pre-selected params.\n",
    "ACTIONS = [-2, -1.6, -0.8, -0.4, -0.2, 0, 0.2, 0.4, 0.8, 1.6, 2]\n",
    "EPISODE_SIZE = 1000\n",
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net(model, params):\n",
    "    filename = params_to_filename(params)\n",
    "    observe = 1000  # Number of frames to observe before training.\n",
    "    epsilon = 1\n",
    "    t = 0\n",
    "    episode_number, episode_reward = 0, 0\n",
    "    train_frames = 1000000  # Number of frames to obserce.\n",
    "#     train_frames = 10000  # Number of frames to obserce.\n",
    "    batchSize = params['batchSize']\n",
    "    buffer = params['buffer']\n",
    "    no_of_episodes = int(train_frames/EPISODE_SIZE)\n",
    "    print 'Executing %d episodes'%(int(train_frames/EPISODE_SIZE))\n",
    "    replay = []  # stores tuples of (S, A, R, S').\n",
    "    loss_log = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    state_array = (np.array([state[0], state[1]])).reshape((1,2))\n",
    "    while t < train_frames:\n",
    "        t += 1\n",
    "        if episode_number == no_of_episodes-1:\n",
    "            env.render()\n",
    "\n",
    "        # Choose an action.\n",
    "        if random.random() < epsilon or t < observe:\n",
    "            action = np.random.randint(len(ACTIONS))  # random\n",
    "        else:\n",
    "            # Get Q values for each action.\n",
    "            qval = model.predict(state, batch_size=1)\n",
    "            action = (np.argmax(qval))  # best\n",
    "        \n",
    "        # Take action, observe new state and get rewards. \n",
    "        new_state, reward, _, _ = env.step([ACTIONS[action]])\n",
    "        new_state_array = (np.array([new_state[0], new_state[1]])).reshape((1,2))\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Experience replay storage.\n",
    "        replay.append((state_array, action, reward, new_state_array))\n",
    "        \n",
    "        # If we're done observing, start training.\n",
    "        if t > observe:\n",
    "\n",
    "            # If we've stored enough in our buffer, pop the oldest.\n",
    "            if len(replay) > buffer:\n",
    "                replay.pop(0)\n",
    "\n",
    "            # Randomly sample our experience replay memory\n",
    "            minibatch = random.sample(replay, batchSize)\n",
    "\n",
    "            # Get training values.\n",
    "            X_train, y_train = process_minibatch(minibatch, model)\n",
    "\n",
    "            # Train the model on this batch.\n",
    "            history = LossHistory()\n",
    "            model.fit(\n",
    "                X_train, y_train, batch_size=batchSize,\n",
    "                nb_epoch=1, verbose=0, callbacks=[history]\n",
    "            )\n",
    "            loss_log.append(history.losses)\n",
    "\n",
    "        # Update the starting state with S'.\n",
    "        state = new_state\n",
    "        state_array = new_state_array\n",
    "        \n",
    "        # TODO: Check whether this is required. Decrement epsilon over time.\n",
    "        if epsilon > 0.1 and t > observe:\n",
    "            epsilon -= (1/train_frames)\n",
    "        \n",
    "        if t % EPISODE_SIZE == 0:\n",
    "            print 'Obtained reward of %s in episode %d'%(episode_reward, episode_number)\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "            episode_number += 1\n",
    "            state = env.reset()\n",
    "            state_array = (np.array([state[0], state[1]])).reshape((1,2))\n",
    "        \n",
    "        # Save the model every 25,000 frames.\n",
    "        if t % 1000 == 0:\n",
    "            model.save_weights('saved-models/' + filename + '-' +\n",
    "                               str(t) + '.h5',\n",
    "                               overwrite=True)\n",
    "            print(\"Saving model %s - %d\" % (filename, t))\n",
    "\n",
    "    # Log results after we're done all frames.\n",
    "    log_results(filename, all_rewards, loss_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_minibatch(minibatch, model):\n",
    "    \"\"\"This does the heavy lifting, aka, the training. It's super jacked.\"\"\"\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    # Loop through our batch and create arrays for X and y\n",
    "    # so that we can fit our model at every step.\n",
    "    for memory in minibatch:\n",
    "        # Get stored values.\n",
    "        old_state_m, action_m, reward_m, new_state_m = memory\n",
    "        # Get prediction on old state.\n",
    "        old_qval = model.predict(old_state_m, batch_size=1)\n",
    "        # Get prediction on new state.\n",
    "        newQ = model.predict(new_state_m, batch_size=1)\n",
    "        # Get our best move. I think?\n",
    "        maxQ = np.max(newQ)\n",
    "        y = np.zeros((1, len(ACTIONS)))\n",
    "        y[:] = old_qval[:]\n",
    "        # Perform the update. TODO: Check if this update is right.\n",
    "        update = (reward_m + (GAMMA * maxQ))\n",
    "        # Update the value for the action we took.\n",
    "        y[0][action_m] = update\n",
    "        X_train.append(old_state_m.reshape(NUM_INPUT,))\n",
    "        y_train.append(y.reshape(len(ACTIONS),))\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def params_to_filename(params):\n",
    "    return str(params['nn'][0]) + '-' + str(params['nn'][1]) + '-' + \\\n",
    "            str(params['batchSize']) + '-' + str(params['buffer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def launch_learn(params):\n",
    "    filename = params_to_filename(params)\n",
    "    print(\"Trying %s\" % filename)\n",
    "    # Make sure we haven't run this one.\n",
    "    if not os.path.isfile('results/sonar-frames/loss_data-' + filename + '.csv'):\n",
    "        # Create file so we don't double test when we run multiple\n",
    "        # instances of the script at the same time.\n",
    "        open('results/sonar-frames/loss_data-' + filename + '.csv', 'a').close()\n",
    "        print(\"Starting test.\")\n",
    "        # Train.\n",
    "        model = neural_net(NUM_INPUT, params['nn'])\n",
    "        train_net(model, params)\n",
    "    else:\n",
    "        print(\"Already tested.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_results(filename, episode_rewards, loss_log):\n",
    "#     # Save the results to a file so we can graph it later.\n",
    "#     with open('results/neural-net/learn_data-' + filename + '.csv', 'w') as data_dump:\n",
    "#         wr = csv.writer(data_dump)\n",
    "#         wr.writerows(data_collect)\n",
    "\n",
    "    with open('results/neural-net/loss_data-' + filename + '.csv', 'w') as lf:\n",
    "        wr = csv.writer(lf)\n",
    "        for loss_item in loss_log:\n",
    "            wr.writerow(loss_item)\n",
    "\n",
    "            \n",
    "    plt.figure()\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.savefig('results/neural-net/episodeRewards-' + filename + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1000 episodes\n",
      "Obtained reward of -7536.85348609 in episode 0\n",
      "Obtained reward of -6519.93394282 in episode 1\n",
      "Obtained reward of -7269.4254469 in episode 2\n",
      "Obtained reward of -4667.22803289 in episode 3\n",
      "Obtained reward of -5621.2576164 in episode 4\n",
      "Obtained reward of -7526.27376597 in episode 5\n",
      "Obtained reward of -7380.56484846 in episode 6\n",
      "Obtained reward of -6073.04236632 in episode 7\n",
      "Obtained reward of -8209.94459547 in episode 8\n",
      "Obtained reward of -8166.42039094 in episode 9\n",
      "Obtained reward of -5183.00758806 in episode 10\n",
      "Obtained reward of -7553.59124189 in episode 11\n",
      "Obtained reward of -5458.30760643 in episode 12\n",
      "Obtained reward of -7909.09020938 in episode 13\n",
      "Obtained reward of -4735.06944119 in episode 14\n",
      "Obtained reward of -7083.2126982 in episode 15\n",
      "Obtained reward of -6631.44029065 in episode 16\n",
      "Obtained reward of -4844.80150471 in episode 17\n",
      "Obtained reward of -6777.65112911 in episode 18\n",
      "Obtained reward of -7903.60880598 in episode 19\n",
      "Obtained reward of -5525.544195 in episode 20\n",
      "Obtained reward of -8381.6070308 in episode 21\n",
      "Obtained reward of -7135.58408398 in episode 22\n",
      "Obtained reward of -6872.69154863 in episode 23\n",
      "Obtained reward of -5504.13275041 in episode 24\n",
      "Saving model 164-150-100-50000 - 25000\n",
      "Obtained reward of -6906.73029628 in episode 25\n",
      "Obtained reward of -6094.20785953 in episode 26\n",
      "Obtained reward of -5037.21222852 in episode 27\n",
      "Obtained reward of -7709.84353814 in episode 28\n",
      "Obtained reward of -6982.3137797 in episode 29\n",
      "Obtained reward of -5613.36488537 in episode 30\n",
      "Obtained reward of -5758.82941794 in episode 31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-09fd488c588a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         }\n\u001b[1;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_INPUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACTIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-a9ca52eaffe1>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(model, params)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Get training values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# Train the model on this batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-0eccc96213ce>\u001b[0m in \u001b[0;36mprocess_minibatch\u001b[0;34m(minibatch, model)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mold_state_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Get prediction on old state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mold_qval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Get prediction on new state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mnewQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asvenk/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asvenk/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1197\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/Users/asvenk/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asvenk/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asvenk/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asvenk/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asvenk/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/asvenk/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asvenk/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if TUNING:\n",
    "        param_list = []\n",
    "        nn_params = [[164, 150], [256, 256],\n",
    "                     [512, 512], [1000, 1000]]\n",
    "        batchSizes = [40, 100, 400]\n",
    "        buffers = [10000, 50000]\n",
    "\n",
    "        for nn_param in nn_params:\n",
    "            for batchSize in batchSizes:\n",
    "                for buffer in buffers:\n",
    "                    params = {\n",
    "                        \"batchSize\": batchSize,\n",
    "                        \"buffer\": buffer,\n",
    "                        \"nn\": nn_param\n",
    "                    }\n",
    "                    param_list.append(params)\n",
    "\n",
    "        for param_set in param_list:\n",
    "            launch_learn(param_set)\n",
    "\n",
    "    else:\n",
    "        nn_param = [164, 150]\n",
    "        params = {\n",
    "            \"batchSize\": 100,\n",
    "            \"buffer\": 50000,\n",
    "            \"nn\": nn_param\n",
    "        }\n",
    "        model = neural_net(NUM_INPUT, len(ACTIONS), nn_param)\n",
    "        train_net(model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
