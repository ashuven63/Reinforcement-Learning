{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from reinforcementLearningCar.nn import neural_net, LossHistory\n",
    "import os.path\n",
    "import timeit\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 12:40:58,079] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space\n",
      "Box(1,)\n",
      "Observation Space\n",
      "Box(3,)\n"
     ]
    }
   ],
   "source": [
    "NUM_INPUT = 2\n",
    "GAMMA = 0.9  # Forgetting.\n",
    "TUNING = False  # If False, just use arbitrary, pre-selected params.\n",
    "ACTIONS = [-2, -1.6, -0.8, -0.4, -0.2, 0, 0.2, 0.4, 0.8, 1.6, 2]\n",
    "EPISODE_SIZE = 500\n",
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net(model, params):\n",
    "    filename = params_to_filename(params)\n",
    "    observe = 1000  # Number of frames to observe before training.\n",
    "    epsilon = 1\n",
    "    t = 0\n",
    "    episode_number, episode_reward = 0, 0\n",
    "#     train_frames = 1000000  # Number of frames to obserce.\n",
    "    train_frames = 10000  # Number of frames to obserce.\n",
    "    batchSize = params['batchSize']\n",
    "    buffer = params['buffer']\n",
    "    print 'Executing %d episodes'%(int(train_frames/EPISODE_SIZE))\n",
    "    replay = []  # stores tuples of (S, A, R, S').\n",
    "    loss_log = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    state_array = (np.array([state[0], state[1]])).reshape((1,2))\n",
    "    while t < train_frames:\n",
    "        t += 1\n",
    "\n",
    "        # Choose an action.\n",
    "        if random.random() < epsilon or t < observe:\n",
    "            action = np.random.randint(len(ACTIONS))  # random\n",
    "        else:\n",
    "            # Get Q values for each action.\n",
    "            qval = model.predict(state, batch_size=1)\n",
    "            action = (np.argmax(qval))  # best\n",
    "        \n",
    "        # Take action, observe new state and get rewards. \n",
    "        new_state, reward, _, _ = env.step([ACTIONS[action]])\n",
    "        new_state_array = (np.array([new_state[0], new_state[1]])).reshape((1,2))\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Experience replay storage.\n",
    "        replay.append((state_array, action, reward, new_state_array))\n",
    "        \n",
    "        # If we're done observing, start training.\n",
    "        if t > observe:\n",
    "\n",
    "            # If we've stored enough in our buffer, pop the oldest.\n",
    "            if len(replay) > buffer:\n",
    "                replay.pop(0)\n",
    "\n",
    "            # Randomly sample our experience replay memory\n",
    "            minibatch = random.sample(replay, batchSize)\n",
    "\n",
    "            # Get training values.\n",
    "            X_train, y_train = process_minibatch(minibatch, model)\n",
    "\n",
    "            # Train the model on this batch.\n",
    "            history = LossHistory()\n",
    "            model.fit(\n",
    "                X_train, y_train, batch_size=batchSize,\n",
    "                nb_epoch=1, verbose=0, callbacks=[history]\n",
    "            )\n",
    "            loss_log.append(history.losses)\n",
    "\n",
    "        # Update the starting state with S'.\n",
    "        state = new_state\n",
    "        state_array = new_state_array\n",
    "        \n",
    "        # TODO: Check whether this is required. Decrement epsilon over time.\n",
    "        if epsilon > 0.1 and t > observe:\n",
    "            epsilon -= (1/train_frames)\n",
    "        \n",
    "        if t % EPISODE_SIZE == 0:\n",
    "            print 'Obtained reward of %s in episode %d'%(episode_reward, episode_number)\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "            episode_number += 1\n",
    "            state = env.reset()\n",
    "            state_array = (np.array([state[0], state[1]])).reshape((1,2))\n",
    "        \n",
    "        # Save the model every 25,000 frames.\n",
    "        if t % 25000 == 0:\n",
    "            model.save_weights('saved-models/' + filename + '-' +\n",
    "                               str(t) + '.h5',\n",
    "                               overwrite=True)\n",
    "            print(\"Saving model %s - %d\" % (filename, t))\n",
    "\n",
    "    # Log results after we're done all frames.\n",
    "    log_results(filename, all_rewards, loss_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_minibatch(minibatch, model):\n",
    "    \"\"\"This does the heavy lifting, aka, the training. It's super jacked.\"\"\"\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    # Loop through our batch and create arrays for X and y\n",
    "    # so that we can fit our model at every step.\n",
    "    for memory in minibatch:\n",
    "        # Get stored values.\n",
    "        old_state_m, action_m, reward_m, new_state_m = memory\n",
    "        # Get prediction on old state.\n",
    "        old_qval = model.predict(old_state_m, batch_size=1)\n",
    "        # Get prediction on new state.\n",
    "        newQ = model.predict(new_state_m, batch_size=1)\n",
    "        # Get our best move. I think?\n",
    "        maxQ = np.max(newQ)\n",
    "        y = np.zeros((1, len(ACTIONS)))\n",
    "        y[:] = old_qval[:]\n",
    "        # Perform the update. TODO: Check if this update is right.\n",
    "        update = (reward_m + (GAMMA * maxQ))\n",
    "        # Update the value for the action we took.\n",
    "        y[0][action_m] = update\n",
    "        X_train.append(old_state_m.reshape(NUM_INPUT,))\n",
    "        y_train.append(y.reshape(len(ACTIONS),))\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def params_to_filename(params):\n",
    "    return str(params['nn'][0]) + '-' + str(params['nn'][1]) + '-' + \\\n",
    "            str(params['batchSize']) + '-' + str(params['buffer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def launch_learn(params):\n",
    "    filename = params_to_filename(params)\n",
    "    print(\"Trying %s\" % filename)\n",
    "    # Make sure we haven't run this one.\n",
    "    if not os.path.isfile('results/sonar-frames/loss_data-' + filename + '.csv'):\n",
    "        # Create file so we don't double test when we run multiple\n",
    "        # instances of the script at the same time.\n",
    "        open('results/sonar-frames/loss_data-' + filename + '.csv', 'a').close()\n",
    "        print(\"Starting test.\")\n",
    "        # Train.\n",
    "        model = neural_net(NUM_INPUT, params['nn'])\n",
    "        train_net(model, params)\n",
    "    else:\n",
    "        print(\"Already tested.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_results(filename, episode_rewards, loss_log):\n",
    "#     # Save the results to a file so we can graph it later.\n",
    "#     with open('results/neural-net/learn_data-' + filename + '.csv', 'w') as data_dump:\n",
    "#         wr = csv.writer(data_dump)\n",
    "#         wr.writerows(data_collect)\n",
    "\n",
    "    with open('results/neural-net/loss_data-' + filename + '.csv', 'w') as lf:\n",
    "        wr = csv.writer(lf)\n",
    "        for loss_item in loss_log:\n",
    "            wr.writerow(loss_item)\n",
    "\n",
    "            \n",
    "    plt.figure()\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.savefig('results/neural-net/episodeRewards-' + filename + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 20 episodes\n",
      "Obtained reward of -2521.0084504 in episode 0\n",
      "Obtained reward of -2523.82650562 in episode 1\n",
      "Obtained reward of -2261.84077474 in episode 2\n",
      "Obtained reward of -3833.52283097 in episode 3\n",
      "Obtained reward of -3570.05785867 in episode 4\n",
      "Obtained reward of -2420.08967737 in episode 5\n",
      "Obtained reward of -3624.6402525 in episode 6\n",
      "Obtained reward of -4103.76315148 in episode 7\n",
      "Obtained reward of -2343.5712407 in episode 8\n",
      "Obtained reward of -2336.5711055 in episode 9\n",
      "Obtained reward of -3269.60823002 in episode 10\n",
      "Obtained reward of -3009.66267591 in episode 11\n",
      "Obtained reward of -2034.20233533 in episode 12\n",
      "Obtained reward of -3185.33456522 in episode 13\n",
      "Obtained reward of -2820.14675142 in episode 14\n",
      "Obtained reward of -2524.26867432 in episode 15\n",
      "Obtained reward of -3336.20271367 in episode 16\n",
      "Obtained reward of -3507.04703184 in episode 17\n",
      "Obtained reward of -2341.12388826 in episode 18\n",
      "Obtained reward of -2207.90664746 in episode 19\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if TUNING:\n",
    "        param_list = []\n",
    "        nn_params = [[164, 150], [256, 256],\n",
    "                     [512, 512], [1000, 1000]]\n",
    "        batchSizes = [40, 100, 400]\n",
    "        buffers = [10000, 50000]\n",
    "\n",
    "        for nn_param in nn_params:\n",
    "            for batchSize in batchSizes:\n",
    "                for buffer in buffers:\n",
    "                    params = {\n",
    "                        \"batchSize\": batchSize,\n",
    "                        \"buffer\": buffer,\n",
    "                        \"nn\": nn_param\n",
    "                    }\n",
    "                    param_list.append(params)\n",
    "\n",
    "        for param_set in param_list:\n",
    "            launch_learn(param_set)\n",
    "\n",
    "    else:\n",
    "        nn_param = [164, 150]\n",
    "        params = {\n",
    "            \"batchSize\": 100,\n",
    "            \"buffer\": 50000,\n",
    "            \"nn\": nn_param\n",
    "        }\n",
    "        model = neural_net(NUM_INPUT, len(ACTIONS), nn_param)\n",
    "        train_net(model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
