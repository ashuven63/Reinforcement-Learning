{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asvenk/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib\n",
    "import math\n",
    "import sys, traceback\n",
    "import logging\n",
    "import pickle\n",
    "import copy\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "Q = defaultdict(dict)\n",
    "Observed_Counts = defaultdict(dict)\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.9\n",
    "NO_OF_EPISODES = 2000\n",
    "NO_OF_ITER = 500\n",
    "ANGLE_TILE_SIZE = 18 # Should be such that no bin lies on both sides of center\n",
    "THETADOT_TILE_SIZE = 6\n",
    "# ACTIONS = [-2, -1.6, -0.8, -0.4, -0.2, 0, 0.2, 0.4, 0.8, 1.6, 2]\n",
    "ACTIONS = [-1.5, -0.1, 0, 0.1, 1.5 ]\n",
    "MAX_LENGTH_MEMORY = 10000\n",
    "REPLAY_SAMPLE_SIZE = 50\n",
    "ENABLE_REPLAY = False\n",
    "DEBUG = False\n",
    "replay_memory = []\n",
    "filename = str(NO_OF_EPISODES) + '_' + str(NO_OF_ITER) + '_' + str(MAX_LENGTH_MEMORY) + '_' + str(REPLAY_SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thetadot_bin = {}\n",
    "def epsilon_greedy(state, epsilon):\n",
    "    # print state\n",
    "    valid_actions = Q[state].keys()\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(ACTIONS)\n",
    "    max_action = max(Q[state], key=Q[state].get)\n",
    "    return max_action\n",
    "\n",
    "def set_thetadot_bins():\n",
    "    thetadot_bin[1] = (0, 0.5)\n",
    "    thetadot_bin[-1] = (-0.5, 0)\n",
    "    thetadot_bin[2] = (0.5, 1.5)\n",
    "    thetadot_bin[3] = (1.5, 2.5)\n",
    "    thetadot_bin[4] = (2.5, 3.5)\n",
    "    thetadot_bin[5] = (3.5, 4.5)\n",
    "    thetadot_bin[6] = (4.5, 5.5)\n",
    "    thetadot_bin[7] = (5.5, 6.5)\n",
    "    thetadot_bin[8] = (6.5, 8.1)\n",
    "    \n",
    "    thetadot_bin[-2] = (-1.5, -0.5)\n",
    "    thetadot_bin[-3] = (-2.5, -1.5)\n",
    "    thetadot_bin[-4] = (-3.5, -2.5)\n",
    "    thetadot_bin[-5] = (-4.5, -3.5)\n",
    "    thetadot_bin[-6] = (-5.5, -4.5)\n",
    "    thetadot_bin[-7] = (-6.5, -5.5)\n",
    "    thetadot_bin[-8] = (-8.1, -6.5)\n",
    "\n",
    "def set_theta_bins():\n",
    "    theta_bin[0] = [(0,10), (350,360)]\n",
    "    theta_bin[1] = [(10,30)]\n",
    "    theta_bin[2] = [(30,50)]\n",
    "    theta_bin[3] = [(50,70)]\n",
    "    theta_bin[4] = [(70,90)]\n",
    "    theta_bin[5] = [(90,110)]\n",
    "    theta_bin[6] = [(110,130)]\n",
    "    theta_bin[7] = [(130,150)]\n",
    "    theta_bin[8] = [(150,170)]\n",
    "    theta_bin[9] = [(170,190)]\n",
    "    theta_bin[10] = [(190,210)]\n",
    "    theta_bin[11] = [(210,230)]\n",
    "    theta_bin[12] = [(230,250)]\n",
    "    theta_bin[13] = [(250,270)]\n",
    "    theta_bin[14] = [(270,290)]\n",
    "    theta_bin[15] = [(290,310)]\n",
    "    theta_bin[16] = [(310,330)]\n",
    "    theta_bin[17] = [(330,350)]    \n",
    "\n",
    "set_thetadot_bins()\n",
    "\n",
    "def get_theta_bin(angle):\n",
    "    bin_num = int(angle/ANGLE_TILE_SIZE)\n",
    "    return bin_num\n",
    "    \n",
    "def get_thetadot_bin(i):\n",
    "    for key in thetadot_bin:\n",
    "        if i > thetadot_bin[key][0] and i <= thetadot_bin[key][1]:\n",
    "            return key\n",
    "    raise \"Key not found for %s\"%(i)\n",
    "\n",
    "def get_discrete_state(S):\n",
    "    theta = get_theta_bin((math.degrees(normalize_angle_360(S[0])))) \n",
    "    thetadot = get_thetadot_bin(S[1])\n",
    "    return (theta, thetadot)\n",
    "\n",
    "def normalize_angle_180(x):\n",
    "    x = (x % (2*np.pi))\n",
    "    if x < 0:\n",
    "        x = ( x + (2*np.pi))\n",
    "    if x >=0 and x <=180:\n",
    "        return x\n",
    "    else:\n",
    "        return x - 360\n",
    "\n",
    "def normalize_angle_360(x):\n",
    "    x = (x % (2*np.pi))\n",
    "    if x < 0:\n",
    "        return ( x + (2*np.pi))\n",
    "    return x\n",
    "    \n",
    "def init_Q():\n",
    "    angle_incr_step = (1.8/(180/ANGLE_TILE_SIZE))\n",
    "    for theta in range(0, 360/ANGLE_TILE_SIZE):\n",
    "        for thetadot in thetadot_bin.keys():\n",
    "            for action in ACTIONS:\n",
    "                if theta > 180:\n",
    "                    Q[(theta, thetadot)][action] = 0.2 + (angle_incr_step * (360 - theta))\n",
    "                else:\n",
    "                    Q[(theta, thetadot)][action] = -(0.2 + (angle_incr_step * (theta)))\n",
    "                Observed_Counts[(theta, thetadot, action)] = 0\n",
    "\n",
    "def replay():\n",
    "    if ENABLE_REPLAY:\n",
    "        # Add observation to replay memory\n",
    "        choices = np.array(replay_memory)\n",
    "        idx = np.random.choice(len(choices), REPLAY_SAMPLE_SIZE)\n",
    "        current_sample_set = choices[idx]\n",
    "        for item in current_sample_set:\n",
    "            replay_A_next = max(Q[item[3]], key=Q[item[3]].get)\n",
    "            Q[item[0]][item[1]] = Q[item[0]][item[1]] + ALPHA * (item[2] + GAMMA*Q[item[3]][replay_A_next] - Q[item[0]][item[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_hist(v, b, name):\n",
    "    plt.figure()\n",
    "    plt.hist(v, bins=b)\n",
    "    # plt.show()\n",
    "    plt.savefig(name)\n",
    "    plt.close()\n",
    "    \n",
    "def plot_episode_rewards(episode_rewards):\n",
    "    plt.figure()\n",
    "    plt.plot(range(NO_OF_EPISODES), episode_rewards)\n",
    "    plt.savefig(\"EpisodeRewards\"+filename)\n",
    "    plt.close()\n",
    "    \n",
    "def smooth_graph(yvalues):\n",
    "    smooth_values = []\n",
    "    for i in range(len(yvalues) - 100):\n",
    "        l = yvalues[i:i+100]\n",
    "        smooth_values.append(sum(l) / 100)\n",
    "    return smooth_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qlearning(cross_eval, ep, ep_d, env_name):\n",
    "    env = gym.make(env_name)\n",
    "    if not cross_eval:\n",
    "        init_Q()\n",
    "    episode_rewards = []\n",
    "    observed_theta, observed_thetadot = [], []\n",
    "    all_observed_theta, all_observed_thetadot = [], []\n",
    "    td_error_all = []\n",
    "    epsilon = ep\n",
    "    epsilon_d = ep_d\n",
    "    for i in range(NO_OF_EPISODES):\n",
    "        if i % 100 == 0:\n",
    "            print 'Episode {0}'.format(i)\n",
    "            if DEBUG:\n",
    "                fn = \"%d\"%(i)\n",
    "                plot_hist(observed_thetadot, range(-8, 9), fn+\"thetadot\")\n",
    "                plot_hist(observed_theta, 360, fn+\"theta\")\n",
    "                observed_theta = []\n",
    "                observed_thetadot = []\n",
    "        S = env.reset()\n",
    "        discrete_S = get_discrete_state(S)\n",
    "        total_episode_reward = 0\n",
    "        for t in range(NO_OF_ITER):\n",
    "            A = epsilon_greedy(discrete_S, epsilon)\n",
    "            if DEBUG:\n",
    "                if i > NO_OF_EPISODES - 6:\n",
    "                    print \"Selected %s Action for State %s with Value %s\"%(A, discrete_S, Q[discrete_S][A])\n",
    "            # Execute the step\n",
    "            if env_name == 'Pendulum-v0':\n",
    "                action = [A]\n",
    "            else:\n",
    "                action = [A]\n",
    "            S_next, reward, _, _ = env.step(action)\n",
    "            total_episode_reward += reward\n",
    "            # If debugging observe the counts of each state. \n",
    "            if DEBUG:\n",
    "                Observed_Counts[(discrete_S[0], discrete_S[1], A)] += 1\n",
    "                observed_theta.append(discrete_S[0])\n",
    "                observed_thetadot.append(S[1])\n",
    "                all_observed_theta.append(discrete_S[0])\n",
    "                all_observed_thetadot.append(S[1])\n",
    "            discrete_S_next = get_discrete_state(S_next)\n",
    "            A_next = max(Q[discrete_S_next], key=Q[discrete_S_next].get)\n",
    "            # Update the Q values\n",
    "            td_error = reward + GAMMA*Q[discrete_S_next][A_next] - Q[discrete_S][A]\n",
    "            td_error_all.append(td_error ** 2)\n",
    "            Q[discrete_S][A] = Q[discrete_S][A] + ALPHA*(td_error)\n",
    "            discrete_S = discrete_S_next\n",
    "            # Experience Replay\n",
    "            if len(replay_memory) == MAX_LENGTH_MEMORY:\n",
    "                replay_memory.pop(0)\n",
    "            replay_memory.append([discrete_S, A, reward, discrete_S_next])\n",
    "            replay()            \n",
    "\n",
    "        # Update the rewards and the no of steps taken\n",
    "        episode_rewards.append(total_episode_reward)\n",
    "        # Epsilon Decay per episode\n",
    "        if i > 1000 and not cross_eval:\n",
    "            epsilon -= (1/i)\n",
    "    return episode_rewards, td_error_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_memory = []\n",
    "REWARD_MEMORY_LENGTH = 100000\n",
    "MIN_OBSERVE_LEN = 5000\n",
    "model = linear_model.SGDRegressor()\n",
    "\n",
    "def qlearning_decay(cross_eval, ep, ep_d, env_name):\n",
    "    env = gym.make(env_name)\n",
    "    if not cross_eval:\n",
    "        init_Q()\n",
    "    episode_rewards, actual_rewards = [], []\n",
    "    observed_theta, observed_thetadot = [], []\n",
    "    all_observed_theta, all_observed_thetadot = [], []\n",
    "    td_error_all = []\n",
    "    epsilon = ep\n",
    "    epsilon_d = ep_d\n",
    "    for i in range(NO_OF_EPISODES):\n",
    "        if i % 100 == 0:\n",
    "            print 'Episode %d'%(i)\n",
    "        S = env.reset()\n",
    "        discrete_S = get_discrete_state(S)\n",
    "        total_episode_reward = 0\n",
    "        actual_episode_reward  = 0\n",
    "        no_received = 0\n",
    "        for t in range(NO_OF_ITER):\n",
    "            A = epsilon_greedy(discrete_S, epsilon)\n",
    "            S_next, reward, _, _ = env.step([A])\n",
    "            if reward == 0: # Did not receive the reward for this time step.\n",
    "#                 print 'Did not receive reward at episode %d and timestep %d'%(i, t)\n",
    "                if len(reward_memory) <= int(MIN_OBSERVE_LEN) and not cross_eval: # Check what should be done here ?\n",
    "                    continue\n",
    "                try:\n",
    "                    reward = get_estimated_reward(np.array([S[0], S[1], A]))\n",
    "                except:\n",
    "                    pass\n",
    "                true_reward = env.get_true_rewards()\n",
    "                actual_episode_reward += true_reward\n",
    "#                 print 'Got an estimated reward of %s'%(reward)\n",
    "                no_received += 1\n",
    "            else:\n",
    "                actual_episode_reward += reward\n",
    "                if len(reward_memory) > REWARD_MEMORY_LENGTH:\n",
    "                    reward_memory.pop(0)\n",
    "                if not cross_eval:\n",
    "                    reward_memory.append(np.array([S[0], S[1], A, reward]))\n",
    "                # add reward to replay memory and retrain the model\n",
    "            total_episode_reward += reward\n",
    "            discrete_S_next = get_discrete_state(S_next)\n",
    "            A_next = max(Q[discrete_S_next], key=Q[discrete_S_next].get)\n",
    "            # Update the Q values\n",
    "            # TODO: Should you update this when you don't get the actual reward.\n",
    "            td_error = reward + GAMMA*Q[discrete_S_next][A_next] - Q[discrete_S][A]\n",
    "            td_error_all.append(td_error**2)\n",
    "            Q[discrete_S][A] = Q[discrete_S][A] + ALPHA*(td_error)\n",
    "            discrete_S = discrete_S_next\n",
    "        episode_rewards.append(total_episode_reward)\n",
    "        actual_rewards.append(actual_episode_reward)\n",
    "        if len(reward_memory) > int(MIN_OBSERVE_LEN):\n",
    "            update_model()\n",
    "#         print 'Did not Revecied rewards in %d steps out of %d steps' % (no_received, NO_OF_ITER)\n",
    "        if i > 1000 and not cross_eval:\n",
    "            epsilon -= (1/i)\n",
    "    print len(reward_memory)\n",
    "    return episode_rewards, td_error_all, actual_rewards\n",
    "\n",
    "def get_estimated_reward(state_feature):\n",
    "    state_feature = state_feature.reshape(1,-1)\n",
    "    return model.predict(state_feature)\n",
    "            \n",
    "def update_model():\n",
    "    minibatch = random.sample(reward_memory, MIN_OBSERVE_LEN)\n",
    "    X, y = [], []\n",
    "    for reward_sample in reward_memory:\n",
    "        X.append(reward_sample[:-1])\n",
    "        y.append(reward_sample[-1])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-26 15:18:53,207] Making new env: SemisuperPendulumRandom-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space\n",
      "Box(1,)\n",
      "Observation Space\n",
      "Box(3,)\n",
      "Episode 0\n",
      "Episode 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ffd36fe56bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrewards_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_error_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_episode_rewards_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqlearning_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SemisuperPendulumRandom-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrewards_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_error_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_episode_rewards_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqlearning_decay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SemisuperPendulumRandom-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-133c5e62bba5>\u001b[0m in \u001b[0;36mqlearning_decay\u001b[0;34m(cross_eval, ep, ep_d, env_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNO_OF_ITER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrete_S\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mS_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Did not receive the reward for this time step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#                 print 'Did not receive reward at episode %d and timestep %d'%(i, t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asvenk/Courses/RL/Reinforcement-Learning/SemiSupervisedRL/gym/gym/envs/safety/semisuper.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# assert self.observation_space.contains(observation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asvenk/Courses/RL/Reinforcement-Learning/SemiSupervisedRL/gym/gym/envs/classic_control/pendulum.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mangle_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mthdot\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.001\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# print \"Th:%s, costs:%s\"%(angle_normalize(th), -costs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mnewthdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthdot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mnewth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewthdot\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mnewthdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewthdot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_speed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_speed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#pylint: disable=E1111\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards_train, td_error_train, actual_episode_rewards_train = qlearning_decay(False, 0.3, 1, 'SemisuperPendulumRandom-v0')\n",
    "rewards_eval, td_error_eval, actual_episode_rewards_eval = qlearning_decay(True, 0, 1, 'SemisuperPendulumRandom-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rewards_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ef6a1aaaa5ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplot_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_episode_rewards_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplot_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_episode_rewards_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rewards_train' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_error(a, b):\n",
    "    error = []\n",
    "    for x,y in zip(a, b):\n",
    "        error.append((x-y)**2)\n",
    "    plt.plot(error)\n",
    "#     plt.show()\n",
    "\n",
    "plot_error(rewards_train, actual_episode_rewards_train)\n",
    "plot_error(rewards_eval, actual_episode_rewards_eval)\n",
    "plt.legend(['train','eval'])\n",
    "plt.show()\n",
    "plt.plot(smooth_graph(rewards_train))\n",
    "plt.plot(smooth_graph(rewards_eval))\n",
    "plt.legend(['train','eval'])\n",
    "plt.show()\n",
    "plt.plot(smooth_graph(td_error_train))\n",
    "plt.plot(smooth_graph(td_error_eval))\n",
    "plt.legend(['train','eval'])\n",
    "plt.show()\n",
    "# plt.plot(rewards_eval)\n",
    "# plt.legend(['train','eval'])\n",
    "# rewards_train_normal, td_error_train_normal = qlearning(False, 0.1, 1, 'SemisuperPendulumDecay-v0')\n",
    "# rewards_eval_normal, td_error_eval_normal = qlearning(True, 0, 1, 'SemisuperPendulumDecay-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(smooth_graph(rewards_train))\n",
    "# plt.plot(smooth_graph(rewards_eval))\n",
    "td_error_train_norm = [x**2 for x in td_error_train]\n",
    "td_error_eval_norm = [x**2 for x in td_error_eval]\n",
    "plt.plot(smooth_graph(td_error_train_norm))\n",
    "plt.plot(smooth_graph(td_error_eval_norm))\n",
    "# plt.plot(smooth_graph(rewards_train_normal))\n",
    "# plt.plot(smooth_graph(rewards_eval_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(smooth_graph(rewards_train))\n",
    "plt.plot(smooth_graph(rewards_eval))\n",
    "\n",
    "\n",
    "# episode_rewards_train, td_error_train = qlearning(False, 0.1, 1)\n",
    "# episode_rewards_eval, td_error_eval = qlearning(True, 0, 1)\n",
    "# plt.plot(smooth_graph(episode_rewards_train))\n",
    "# plt.plot(smooth_graph(episode_rewards_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_pickle_dump(q_name, oc_name):\n",
    "    pickle.dump(Q, open(q_name, \"wb\"))\n",
    "    pickle.dump(Observed_Counts, open(oc_name, \"wb\"))\n",
    "    \n",
    "def load_pickle_dump(q_name, oc_name):\n",
    "    observed_counts = pickle.load( open(q_name, \"rb\" ) )\n",
    "    Q = pickle.load( open(oc_name, \"rb\" ) )\n",
    "    return observed_counts, Q\n",
    "\n",
    "def render(q, oc ,env_name):\n",
    "    Q, _ = load_pickle_dump(q, oc)\n",
    "    env = gym.make(env_name)\n",
    "    S = env.reset()\n",
    "    env.render()\n",
    "    for t in range(NO_OF_ITER):\n",
    "        dis_S = get_discrete_state(S)\n",
    "        max_action = max(Q[dis_S], key=Q[dis_S].get)\n",
    "        S, reward, _, _ = env.step([max_action])\n",
    "        env.render()\n",
    "\n",
    "def evaluation(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    episode_rewards = []\n",
    "    for i in range(NO_OF_EPISODES):\n",
    "        S = env.reset()\n",
    "        episode_reward = 0\n",
    "#         env.render()\n",
    "        for t in range(NO_OF_ITER):\n",
    "            dis_S = get_discrete_state(S)\n",
    "            max_action = max(Q[dis_S], key=Q[dis_S].get)\n",
    "            S, reward, _, _ = env.step([max_action])\n",
    "            if reward == 0:\n",
    "                conti\n",
    "            episode_reward += reward\n",
    "#             env.render()\n",
    "        episode_rewards.append(episode_reward)\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# environments = ['Pendulum-v0','SemisuperPendulumNoise-v0','SemisuperPendulumRandom-v0','SemisuperPendulumDecay-v0']\n",
    "environments = ['Pendulum-v0']\n",
    "def plot_reward_environments(train, test, legend):\n",
    "    all_legend = []\n",
    "    for reward, l in zip(train,legend):\n",
    "        plt.plot(smooth_graph(reward))\n",
    "        all_legend.append(l+'_train')\n",
    "    for reward, l in zip(test,legend):\n",
    "        plt.plot(smooth_graph(reward))\n",
    "        all_legend.append(l+'_eval')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.legend(all_legend, loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "def test_environments():\n",
    "#     environments = ['Pendulum-v0','SemisuperPendulumNoise-v0','SemisuperPendulumRandom-v0','SemisuperPendulumDecay-v0']\n",
    "    environments = ['Pendulum-v0']\n",
    "    all_episodes_train, all_episodes_eval = [], []\n",
    "    all_td_error_train, all_td_error_eval = [], []\n",
    "    for env in environments:\n",
    "        print 'Testing '+ env\n",
    "        episode_rewards_train, td_error_train = qlearning(False, 0.1, 1, env)\n",
    "        save_pickle_dump(\"pickle_dumps/Q\" + filename + \"_\" + env + \".p\", \"ObservedCounts\" + filename + '_' + env + \".p\")\n",
    "#         episode_rewards_eval, td_error_eval = evalution(True, 0, 1, env)\n",
    "#         all_episodes_train.append(copy.deepcopy(episode_rewards_train))\n",
    "#         all_episodes_eval.append(copy.deepcopy(episode_rewards_eval))\n",
    "#         all_td_error_train.append(copy.deepcopy(td_error_train))\n",
    "#         all_td_error_eval.append(copy.deepcopy(td_error_eval))\n",
    "#     plot_reward_environments(all_episodes_train, all_episodes_eval, environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-26 15:45:04,696] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Pendulum-v0\n",
      "Action Space\n",
      "Box(1,)\n",
      "Observation Space\n",
      "Box(3,)\n",
      "Episode 0\n",
      "Episode 100\n",
      "Episode 200\n",
      "Episode 300\n",
      "Episode 400\n",
      "Episode 500\n",
      "Episode 600\n",
      "Episode 700\n",
      "Episode 800\n",
      "Episode 900\n",
      "Episode 1000\n",
      "Episode 1100\n",
      "Episode 1200\n",
      "Episode 1300\n",
      "Episode 1400\n",
      "Episode 1500\n",
      "Episode 1600\n",
      "Episode 1700\n",
      "Episode 1800\n",
      "Episode 1900\n"
     ]
    }
   ],
   "source": [
    "test_environments()\n",
    "# environments = ['Pendulum-v0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-26 15:23:52,369] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space\n",
      "Box(1,)\n",
      "Observation Space\n",
      "Box(3,)\n"
     ]
    }
   ],
   "source": [
    "for env in environments:\n",
    "    render(\"pickle_dumps/Q\" + filename + \"_\" + env + \".p\", \"ObservedCounts\" + filename + '_' + env + \".p\", env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_Q_values(Q):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    w = 360/ANGLE_TILE_SIZE\n",
    "    h = len(thetadot_bin.keys())\n",
    "    grid = [[0 for x in range(w)] for y in range(h)] \n",
    "    for theta in range(0, 360/ANGLE_TILE_SIZE):\n",
    "        for thetadot in thetadot_bin.keys():\n",
    "            state = (theta, thetadot)\n",
    "            a = max(Q[state], key=Q[state].get)\n",
    "            ax.scatter(theta, thetadot, a, zdir='z')\n",
    "    plt.show()\n",
    "Q, _ = load_pickle_dump(\"Q\" + filename + \"_\" + env + \".p\", \"ObservedCounts\" + filename + '_' + env + \".p\")\n",
    "# print Q\n",
    "plot_Q_values(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
