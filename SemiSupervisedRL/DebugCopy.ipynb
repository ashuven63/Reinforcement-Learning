{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib\n",
    "import math\n",
    "import sys, traceback\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-21 10:44:57,161] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space\n",
      "Box(1,)\n",
      "Observation Space\n",
      "Box(3,)\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "Q = defaultdict(dict)\n",
    "Observed_Counts = defaultdict(dict)\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.9\n",
    "NO_OF_EPISODES = 10000\n",
    "NO_OF_ITER = 500\n",
    "ANGLE_TILE_SIZE = 18 # Should be such that no bin lies on both sides of center\n",
    "THETADOT_TILE_SIZE = 6\n",
    "env = gym.make('Pendulum-v0')\n",
    "ACTIONS = [-2, -1.6, -0.8, -0.4, -0.2, 0, 0.2, 0.4, 0.8, 1.6, 2]\n",
    "VELOCITY = [x for x in range(-8, 9)]\n",
    "MAX_LENGTH_MEMORY = 10000\n",
    "REPLAY_SAMPLE_SIZE = 2000\n",
    "ENABLE_REPLAY = True\n",
    "RENDER = False\n",
    "DEBUG = False\n",
    "replay_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thetadot_bin = {}\n",
    "def epsilon_greedy(state, epsilon):\n",
    "    # print state\n",
    "    valid_actions = Q[state].keys()\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(ACTIONS)\n",
    "    max_action = max(Q[state], key=Q[state].get)\n",
    "    return max_action\n",
    "\n",
    "def set_thetadot_bins():\n",
    "    thetadot_bin[1] = (0, 0.5)\n",
    "    thetadot_bin[-1] = (-0.5, 0)\n",
    "    thetadot_bin[2] = (0.5, 1.5)\n",
    "    thetadot_bin[3] = (1.5, 2.5)\n",
    "    thetadot_bin[4] = (2.5, 3.5)\n",
    "    thetadot_bin[5] = (3.5, 4.5)\n",
    "    thetadot_bin[6] = (4.5, 5.5)\n",
    "    thetadot_bin[7] = (5.5, 6.5)\n",
    "    thetadot_bin[8] = (6.5, 8.1)\n",
    "    \n",
    "    thetadot_bin[-2] = (-1.5, -0.5)\n",
    "    thetadot_bin[-3] = (-2.5, -1.5)\n",
    "    thetadot_bin[-4] = (-3.5, -2.5)\n",
    "    thetadot_bin[-5] = (-4.5, -3.5)\n",
    "    thetadot_bin[-6] = (-5.5, -4.5)\n",
    "    thetadot_bin[-7] = (-6.5, -5.5)\n",
    "    thetadot_bin[-8] = (-8.1, -6.5)\n",
    "\n",
    "set_thetadot_bins()\n",
    "\n",
    "def get_theta_bin(angle):\n",
    "    bin_num = int(angle/ANGLE_TILE_SIZE)\n",
    "    return bin_num\n",
    "    \n",
    "def get_thetadot_bin(i):\n",
    "    for key in thetadot_bin:\n",
    "        if i > thetadot_bin[key][0] and i <= thetadot_bin[key][1]:\n",
    "            return key\n",
    "    raise \"Key not found for %s\"%(i)\n",
    "\n",
    "# print get_thetadot_bin(8)\n",
    "# print get_thetadot_bin(-8)\n",
    "# print get_thetadot_bin(-7.5)\n",
    "# print get_thetadot_bin(7.5)\n",
    "# print get_thetadot_bin(0)\n",
    "# print get_thetadot_bin(0.2)\n",
    "# print get_thetadot_bin(-0.5)\n",
    "# print get_thetadot_bin(-0.533)\n",
    "# print get_thetadot_bin(-0.4)\n",
    "    \n",
    "def get_discrete_state(S):\n",
    "    theta = get_theta_bin((math.degrees(normalize_angle(S[0])))) \n",
    "    thetadot = get_thetadot_bin(S[1])\n",
    "    return (theta, thetadot)\n",
    "\n",
    "def normalize_angle(x):\n",
    "    x = (x % (2*np.pi))\n",
    "    if x < 0:\n",
    "        return ( x + (2*np.pi))\n",
    "    return x\n",
    "\n",
    "def init_Q():\n",
    "    angle_incr_step = (1.8/(180/ANGLE_TILE_SIZE))\n",
    "    for theta in range(0, 360/ANGLE_TILE_SIZE):\n",
    "        for thetadot in thetadot_bin.keys():\n",
    "            for action in ACTIONS:\n",
    "                if theta > 180:\n",
    "                    Q[(theta, thetadot)][action] = 0.2 + (angle_incr_step * (360 - theta))\n",
    "                else:\n",
    "                    Q[(theta, thetadot)][action] = -(0.2 + (angle_incr_step * (theta)))\n",
    "                Observed_Counts[(theta, thetadot, action)] = 0\n",
    "\n",
    "def replay():\n",
    "    if ENABLE_REPLAY:\n",
    "        # Add observation to replay memory\n",
    "        choices = np.array(replay_memory)\n",
    "        idx = np.random.choice(len(choices), REPLAY_SAMPLE_SIZE)\n",
    "        current_sample_set = choices[idx]\n",
    "        for item in current_sample_set:\n",
    "            replay_A_next = max(Q[item[3]], key=Q[item[3]].get)\n",
    "            Q[item[0]][item[1]] = Q[item[0]][item[1]] + ALPHA * (item[2] + GAMMA*Q[item[3]][replay_A_next] - Q[item[0]][item[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_hist(v, b, name):\n",
    "    plt.figure()\n",
    "    plt.hist(v, bins=b)\n",
    "    # plt.show()\n",
    "    plt.savefig(name)\n",
    "    plt.close()\n",
    "    \n",
    "def plot_episode_rewards(episode_rewards):\n",
    "    plt.figure()\n",
    "    plt.plot(range(NO_OF_EPISODES), episode_rewards)\n",
    "    plt.savefig(\"EpisodeRewards\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qlearning():\n",
    "    init_Q()\n",
    "    episode_rewards = []\n",
    "    observed_theta, observed_thetadot = [], []\n",
    "    all_observed_theta, all_observed_thetadot = [], []\n",
    "    epsilon = 0.1\n",
    "    epsilon_d = 1\n",
    "    for i in range(NO_OF_EPISODES):\n",
    "        print 'Episode {0}'.format(i)\n",
    "        if i % 100 == 0:\n",
    "#             logger.debug('Episode {0}'.format(i))\n",
    "            if DEBUG:\n",
    "                fn = \"%d\"%(i)\n",
    "                plot_hist(observed_thetadot, range(-8, 9), fn+\"thetadot\")\n",
    "                plot_hist(observed_theta, 360, fn+\"theta\")\n",
    "                observed_theta = []\n",
    "                observed_thetadot = []\n",
    "        S = env.reset()\n",
    "        discrete_S = get_discrete_state(S)\n",
    "        total_episode_reward = 0\n",
    "        for t in range(NO_OF_ITER):\n",
    "            if RENDER:\n",
    "                if i == NO_OF_EPISODES -1 :\n",
    "                    env.render() \n",
    "            A = epsilon_greedy(discrete_S, epsilon)\n",
    "            if DEBUG:\n",
    "                if i > NO_OF_EPISODES - 6:\n",
    "                    print \"Selected %s Action for State %s with Value %s\"%(A, discrete_S, Q[discrete_S][A])\n",
    "            # Execute the step\n",
    "            S_next, reward, _, _ = env.step([A])\n",
    "            total_episode_reward += reward\n",
    "            # If debugging observe the counts of each state. \n",
    "            if DEBUG:\n",
    "                Observed_Counts[(discrete_S[0], discrete_S[1], A)] += 1\n",
    "                observed_theta.append(discrete_S[0])\n",
    "                observed_thetadot.append(S[1])\n",
    "                all_observed_theta.append(discrete_S[0])\n",
    "                all_observed_thetadot.append(S[1])\n",
    "            discrete_S_next = get_discrete_state(S_next)\n",
    "            A_next = max(Q[discrete_S_next], key=Q[discrete_S_next].get)\n",
    "            # Update the Q values\n",
    "            Q[discrete_S][A] = Q[discrete_S][A] + ALPHA*(reward + GAMMA*Q[discrete_S_next][A_next] - Q[discrete_S][A])\n",
    "            discrete_S = discrete_S_next\n",
    "            # Experience Replay\n",
    "            if len(replay_memory) == MAX_LENGTH_MEMORY:\n",
    "                replay_memory.pop(0)\n",
    "            replay_memory.append([discrete_S, A, reward, discrete_S_next])\n",
    "            replay()            \n",
    "\n",
    "        if DEBUG:\n",
    "            if i > NO_OF_EPISODES - 6:\n",
    "                print \"************************************************\"\n",
    "        # Update the rewards and the no of steps taken\n",
    "        episode_rewards.append(total_episode_reward)\n",
    "        # Epsilon Decay per episode\n",
    "        epsilon = epsilon/epsilon_d\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Episode 1\n",
      "Episode 2\n",
      "Episode 3\n",
      "Episode 4\n",
      "Episode 5\n",
      "Episode 6\n",
      "Episode 7\n",
      "Episode 8\n",
      "Episode 9\n",
      "Episode 10\n",
      "Episode 11\n",
      "Episode 12\n",
      "Episode 13\n",
      "Episode 14\n",
      "Episode 15\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = qlearning()\n",
    "plot_episode_rewards(episode_rewards)\n",
    "pickle.dump(Q, open(\"Q.p\", \"wb\"))\n",
    "pickle.dump(Observed_Counts, open(\"ObservedCounts.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_episode_rewards(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # for key in Observed_Counts:\n",
    "    #     print key\n",
    "    #     print Observed_Counts[key]\n",
    "    #     print '***********************************************'\n",
    "    # print '############################################'\n",
    "    # for key in \n",
    "    # plt.hist(observed_thetadot, bins=range(-8, 9))\n",
    "    # plt.show()\n",
    "    # plt.hist(observed_theta, bins=360)\n",
    "    # plt.show()\n",
    "    for key in Q:\n",
    "        if (key[0] >= 0 and key[0] <=10) or (key[0] >=350):\n",
    "            print key\n",
    "            print Q[key]\n",
    "    print \"********************************************************************************************************************************************\"\n",
    "    for key in Observed_Counts:\n",
    "        if (key[0] >= 0 and key[0] <=10) or (key[0] >=350):\n",
    "            print \"Key: %s, Value:%s\" % (key, Observed_Counts[key])\n",
    "    # fn = \"all\"\n",
    "#     plot_hist(all_observed_thetadot, range(-8, 9), fn+\"thetadot\")\n",
    "#     plot_hist(all_observed_theta, 360, fn+\"theta\")\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
