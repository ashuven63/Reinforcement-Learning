{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib\n",
    "import math\n",
    "import sys, traceback\n",
    "import logging\n",
    "import pickle\n",
    "import copy\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "Q = defaultdict(dict)\n",
    "Observed_Counts = defaultdict(dict)\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.9\n",
    "NO_OF_EPISODES = 2000\n",
    "NO_OF_ITER = 500\n",
    "ANGLE_TILE_SIZE = 18 # Should be such that no bin lies on both sides of center\n",
    "THETADOT_TILE_SIZE = 6\n",
    "# ACTIONS = [-2, -1.6, -0.8, -0.4, -0.2, 0, 0.2, 0.4, 0.8, 1.6, 2]\n",
    "ACTIONS = [-1.5, -0.1, 0, 0.1, 1.5 ]\n",
    "MAX_LENGTH_MEMORY = 10000\n",
    "REPLAY_SAMPLE_SIZE = 50\n",
    "ENABLE_REPLAY = False\n",
    "DEBUG = False\n",
    "replay_memory = []\n",
    "filename = 'Q' + str(NO_OF_EPISODES) + '_' + str(NO_OF_ITER) + '_' + str(MAX_LENGTH_MEMORY) + '_' + str(REPLAY_SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thetadot_bin = {}\n",
    "def epsilon_greedy(state, epsilon):\n",
    "    # print state\n",
    "    valid_actions = Q[state].keys()\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(ACTIONS)\n",
    "    max_action = max(Q[state], key=Q[state].get)\n",
    "    return max_action\n",
    "\n",
    "def set_thetadot_bins():\n",
    "    thetadot_bin[1] = (0, 0.5)\n",
    "    thetadot_bin[-1] = (-0.5, 0)\n",
    "    thetadot_bin[2] = (0.5, 1.5)\n",
    "    thetadot_bin[3] = (1.5, 2.5)\n",
    "    thetadot_bin[4] = (2.5, 3.5)\n",
    "    thetadot_bin[5] = (3.5, 4.5)\n",
    "    thetadot_bin[6] = (4.5, 5.5)\n",
    "    thetadot_bin[7] = (5.5, 6.5)\n",
    "    thetadot_bin[8] = (6.5, 8.1)\n",
    "    \n",
    "    thetadot_bin[-2] = (-1.5, -0.5)\n",
    "    thetadot_bin[-3] = (-2.5, -1.5)\n",
    "    thetadot_bin[-4] = (-3.5, -2.5)\n",
    "    thetadot_bin[-5] = (-4.5, -3.5)\n",
    "    thetadot_bin[-6] = (-5.5, -4.5)\n",
    "    thetadot_bin[-7] = (-6.5, -5.5)\n",
    "    thetadot_bin[-8] = (-8.1, -6.5)\n",
    "\n",
    "def set_theta_bins():\n",
    "    theta_bin[0] = [(0,10), (350,360)]\n",
    "    theta_bin[1] = [(10,30)]\n",
    "    theta_bin[2] = [(30,50)]\n",
    "    theta_bin[3] = [(50,70)]\n",
    "    theta_bin[4] = [(70,90)]\n",
    "    theta_bin[5] = [(90,110)]\n",
    "    theta_bin[6] = [(110,130)]\n",
    "    theta_bin[7] = [(130,150)]\n",
    "    theta_bin[8] = [(150,170)]\n",
    "    theta_bin[9] = [(170,190)]\n",
    "    theta_bin[10] = [(190,210)]\n",
    "    theta_bin[11] = [(210,230)]\n",
    "    theta_bin[12] = [(230,250)]\n",
    "    theta_bin[13] = [(250,270)]\n",
    "    theta_bin[14] = [(270,290)]\n",
    "    theta_bin[15] = [(290,310)]\n",
    "    theta_bin[16] = [(310,330)]\n",
    "    theta_bin[17] = [(330,350)]    \n",
    "\n",
    "set_thetadot_bins()\n",
    "\n",
    "def get_theta_bin(angle):\n",
    "    bin_num = int(angle/ANGLE_TILE_SIZE)\n",
    "    return bin_num\n",
    "    \n",
    "def get_thetadot_bin(i):\n",
    "    for key in thetadot_bin:\n",
    "        if i > thetadot_bin[key][0] and i <= thetadot_bin[key][1]:\n",
    "            return key\n",
    "    raise \"Key not found for %s\"%(i)\n",
    "\n",
    "def get_discrete_state(S):\n",
    "    theta = get_theta_bin((math.degrees(normalize_angle_360(S[0])))) \n",
    "    thetadot = get_thetadot_bin(S[1])\n",
    "    return (theta, thetadot)\n",
    "\n",
    "def normalize_angle_180(x):\n",
    "    x = (x % (2*np.pi))\n",
    "    if x < 0:\n",
    "        x = ( x + (2*np.pi))\n",
    "    if x >=0 and x <=180:\n",
    "        return x\n",
    "    else:\n",
    "        return x - 360\n",
    "\n",
    "def normalize_angle_360(x):\n",
    "    x = (x % (2*np.pi))\n",
    "    if x < 0:\n",
    "        return ( x + (2*np.pi))\n",
    "    return x\n",
    "    \n",
    "def init_Q():\n",
    "    angle_incr_step = (1.8/(180/ANGLE_TILE_SIZE))\n",
    "    for theta in range(0, 360/ANGLE_TILE_SIZE):\n",
    "        for thetadot in thetadot_bin.keys():\n",
    "            for action in ACTIONS:\n",
    "                if theta > 180:\n",
    "                    Q[(theta, thetadot)][action] = 0.2 + (angle_incr_step * (360 - theta))\n",
    "                else:\n",
    "                    Q[(theta, thetadot)][action] = -(0.2 + (angle_incr_step * (theta)))\n",
    "                Observed_Counts[(theta, thetadot, action)] = 0\n",
    "\n",
    "def replay():\n",
    "    if ENABLE_REPLAY:\n",
    "        # Add observation to replay memory\n",
    "        choices = np.array(replay_memory)\n",
    "        idx = np.random.choice(len(choices), REPLAY_SAMPLE_SIZE)\n",
    "        current_sample_set = choices[idx]\n",
    "        for item in current_sample_set:\n",
    "            replay_A_next = max(Q[item[3]], key=Q[item[3]].get)\n",
    "            Q[item[0]][item[1]] = Q[item[0]][item[1]] + ALPHA * (item[2] + GAMMA*Q[item[3]][replay_A_next] - Q[item[0]][item[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_hist(v, b, name):\n",
    "    plt.figure()\n",
    "    plt.hist(v, bins=b)\n",
    "    # plt.show()\n",
    "    plt.savefig(name)\n",
    "    plt.close()\n",
    "    \n",
    "def plot_episode_rewards(episode_rewards):\n",
    "    plt.figure()\n",
    "    plt.plot(range(NO_OF_EPISODES), episode_rewards)\n",
    "    plt.savefig(\"EpisodeRewards\"+filename)\n",
    "    plt.close()\n",
    "    \n",
    "def smooth_graph(yvalues):\n",
    "    smooth_values = []\n",
    "    for i in range(len(yvalues) - 100):\n",
    "        l = yvalues[i:i+100]\n",
    "        smooth_values.append(sum(l) / 100)\n",
    "    return smooth_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qlearning(cross_eval, ep, ep_d, env_name):\n",
    "    env = gym.make(env_name)\n",
    "    if not cross_eval:\n",
    "        init_Q()\n",
    "    episode_rewards = []\n",
    "    observed_theta, observed_thetadot = [], []\n",
    "    all_observed_theta, all_observed_thetadot = [], []\n",
    "    td_error_all = []\n",
    "    epsilon = ep\n",
    "    epsilon_d = ep_d\n",
    "    for i in range(NO_OF_EPISODES):\n",
    "        if i % 100 == 0:\n",
    "            print 'Episode {0}'.format(i)\n",
    "            if DEBUG:\n",
    "                fn = \"%d\"%(i)\n",
    "                plot_hist(observed_thetadot, range(-8, 9), fn+\"thetadot\")\n",
    "                plot_hist(observed_theta, 360, fn+\"theta\")\n",
    "                observed_theta = []\n",
    "                observed_thetadot = []\n",
    "        S = env.reset()\n",
    "        discrete_S = get_discrete_state(S)\n",
    "        total_episode_reward = 0\n",
    "        for t in range(NO_OF_ITER):\n",
    "            A = epsilon_greedy(discrete_S, epsilon)\n",
    "            if DEBUG:\n",
    "                if i > NO_OF_EPISODES - 6:\n",
    "                    print \"Selected %s Action for State %s with Value %s\"%(A, discrete_S, Q[discrete_S][A])\n",
    "            # Execute the step\n",
    "            if env_name == 'Pendulum-v0':\n",
    "                action = [A]\n",
    "            else:\n",
    "                action = [A]\n",
    "            S_next, reward, _, _ = env.step(action)\n",
    "#             if reward == 0:\n",
    "#                 continue\n",
    "            total_episode_reward += reward\n",
    "            # If debugging observe the counts of each state. \n",
    "            if DEBUG:\n",
    "                Observed_Counts[(discrete_S[0], discrete_S[1], A)] += 1\n",
    "                observed_theta.append(discrete_S[0])\n",
    "                observed_thetadot.append(S[1])\n",
    "                all_observed_theta.append(discrete_S[0])\n",
    "                all_observed_thetadot.append(S[1])\n",
    "            discrete_S_next = get_discrete_state(S_next)\n",
    "            A_next = max(Q[discrete_S_next], key=Q[discrete_S_next].get)\n",
    "            # Update the Q values\n",
    "            td_error = reward + GAMMA*Q[discrete_S_next][A_next] - Q[discrete_S][A]\n",
    "            td_error_all.append(td_error ** 2)\n",
    "            Q[discrete_S][A] = Q[discrete_S][A] + ALPHA*(td_error)\n",
    "            discrete_S = discrete_S_next\n",
    "            # Experience Replay\n",
    "            if len(replay_memory) == MAX_LENGTH_MEMORY:\n",
    "                replay_memory.pop(0)\n",
    "            replay_memory.append([discrete_S, A, reward, discrete_S_next])\n",
    "            replay()            \n",
    "\n",
    "        # Update the rewards and the no of steps taken\n",
    "        episode_rewards.append(total_episode_reward)\n",
    "        # Epsilon Decay per episode\n",
    "        if i > 1000 and not cross_eval:\n",
    "            epsilon -= (1/i)\n",
    "    return episode_rewards, td_error_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_memory = []\n",
    "REWARD_MEMORY_LENGTH = 100000\n",
    "MIN_OBSERVE_LEN = 5000\n",
    "model = linear_model.SGDRegressor()\n",
    "\n",
    "def qlearning_decay(cross_eval, ep, ep_d, env_name):\n",
    "    env = gym.make(env_name)\n",
    "    if not cross_eval:\n",
    "        init_Q()\n",
    "    episode_rewards, actual_rewards = [], []\n",
    "    observed_theta, observed_thetadot = [], []\n",
    "    all_observed_theta, all_observed_thetadot = [], []\n",
    "    td_error_all = []\n",
    "    epsilon = ep\n",
    "    epsilon_d = ep_d\n",
    "    for i in range(NO_OF_EPISODES):\n",
    "        if i % 100 == 0:\n",
    "            print 'Episode %d'%(i)\n",
    "        S = env.reset()\n",
    "        discrete_S = get_discrete_state(S)\n",
    "        total_episode_reward = 0\n",
    "        actual_episode_reward  = 0\n",
    "        no_received = 0\n",
    "        for t in range(NO_OF_ITER):\n",
    "            A = epsilon_greedy(discrete_S, epsilon)\n",
    "            S_next, reward, _, _ = env.step([A])\n",
    "            if reward == 0: # Did not receive the reward for this time step.\n",
    "#                 print 'Did not receive reward at episode %d and timestep %d'%(i, t)\n",
    "                if len(reward_memory) <= int(MIN_OBSERVE_LEN) and not cross_eval: # Check what should be done here ?\n",
    "                    continue\n",
    "                try:\n",
    "                    reward = get_estimated_reward(np.array([S[0], S[1], A]))\n",
    "                except:\n",
    "                    pass\n",
    "                true_reward = env.get_true_rewards()\n",
    "                actual_episode_reward += true_reward\n",
    "#                 print 'Got an estimated reward of %s'%(reward)\n",
    "                no_received += 1\n",
    "            else:\n",
    "                actual_episode_reward += reward\n",
    "                if len(reward_memory) > REWARD_MEMORY_LENGTH:\n",
    "                    reward_memory.pop(0)\n",
    "                if not cross_eval:\n",
    "                    reward_memory.append(np.array([S[0], S[1], A, reward]))\n",
    "                # add reward to replay memory and retrain the model\n",
    "            total_episode_reward += reward\n",
    "            discrete_S_next = get_discrete_state(S_next)\n",
    "            A_next = max(Q[discrete_S_next], key=Q[discrete_S_next].get)\n",
    "            # Update the Q values\n",
    "            # TODO: Should you update this when you don't get the actual reward.\n",
    "            td_error = reward + GAMMA*Q[discrete_S_next][A_next] - Q[discrete_S][A]\n",
    "            td_error_all.append(td_error**2)\n",
    "            Q[discrete_S][A] = Q[discrete_S][A] + ALPHA*(td_error)\n",
    "            discrete_S = discrete_S_next\n",
    "        episode_rewards.append(total_episode_reward)\n",
    "        actual_rewards.append(actual_episode_reward)\n",
    "        if len(reward_memory) > int(MIN_OBSERVE_LEN):\n",
    "            update_model()\n",
    "#         print 'Did not Revecied rewards in %d steps out of %d steps' % (no_received, NO_OF_ITER)\n",
    "        if i > 1000 and not cross_eval:\n",
    "            epsilon -= (1/i)\n",
    "    print len(reward_memory)\n",
    "    return episode_rewards, td_error_all, actual_rewards\n",
    "\n",
    "def get_estimated_reward(state_feature):\n",
    "    state_feature = state_feature.reshape(1,-1)\n",
    "    return model.predict(state_feature)\n",
    "            \n",
    "def update_model():\n",
    "    minibatch = random.sample(reward_memory, MIN_OBSERVE_LEN)\n",
    "    X, y = [], []\n",
    "    for reward_sample in reward_memory:\n",
    "        X.append(reward_sample[:-1])\n",
    "        y.append(reward_sample[-1])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rewards_train, td_error_train, actual_episode_rewards_train = qlearning_decay(False, 0.3, 1, 'SemisuperPendulumRandom-v0')\n",
    "# rewards_eval, td_error_eval, actual_episode_rewards_eval = qlearning_decay(True, 0, 1, 'SemisuperPendulumRandom-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_error(a, b):\n",
    "    error = []\n",
    "    for x,y in zip(a, b):\n",
    "        error.append((x-y)**2)\n",
    "    plt.plot(error)\n",
    "#     plt.show()\n",
    "\n",
    "# plot_error(rewards_train, actual_episode_rewards_train)\n",
    "# plot_error(rewards_eval, actual_episode_rewards_eval)\n",
    "# plt.legend(['train','eval'])\n",
    "# plt.show()\n",
    "# plt.plot(smooth_graph(rewards_train))\n",
    "# plt.plot(smooth_graph(rewards_eval))\n",
    "# plt.legend(['train','eval'])\n",
    "# plt.show()\n",
    "# plt.plot(smooth_graph(td_error_train))\n",
    "# plt.plot(smooth_graph(td_error_eval))\n",
    "# plt.legend(['train','eval'])\n",
    "# plt.show()\n",
    "# plt.plot(rewards_eval)\n",
    "# plt.legend(['train','eval'])\n",
    "# rewards_train_normal, td_error_train_normal = qlearning(False, 0.1, 1, 'SemisuperPendulumDecay-v0')\n",
    "# rewards_eval_normal, td_error_eval_normal = qlearning(True, 0, 1, 'SemisuperPendulumDecay-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(smooth_graph(rewards_train))\n",
    "# plt.plot(smooth_graph(rewards_eval))\n",
    "# td_error_train_norm = [x**2 for x in td_error_train]\n",
    "# td_error_eval_norm = [x**2 for x in td_error_eval]\n",
    "# plt.plot(smooth_graph(td_error_train_norm))\n",
    "# plt.plot(smooth_graph(td_error_eval_norm))\n",
    "# plt.plot(smooth_graph(rewards_train_normal))\n",
    "# plt.plot(smooth_graph(rewards_eval_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(smooth_graph(rewards_train))\n",
    "# plt.plot(smooth_graph(rewards_eval))\n",
    "# episode_rewards_train, td_error_train = qlearning(False, 0.1, 1)\n",
    "# episode_rewards_eval, td_error_eval = qlearning(True, 0, 1)\n",
    "# plt.plot(smooth_graph(episode_rewards_train))\n",
    "# plt.plot(smooth_graph(episode_rewards_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_pickle_dump(q_name, oc_name):\n",
    "    pickle.dump(Q, open(q_name, \"wb\"))\n",
    "    pickle.dump(Observed_Counts, open(oc_name, \"wb\"))\n",
    "    \n",
    "def load_pickle_dump(q_name, oc_name):\n",
    "    observed_counts = pickle.load( open(q_name, \"rb\" ) )\n",
    "    Q = pickle.load( open(oc_name, \"rb\" ) )\n",
    "    return observed_counts, Q\n",
    "\n",
    "def render(q, oc ,env_name):\n",
    "    Q, _ = load_pickle_dump(q, oc)\n",
    "    env = gym.make(env_name)\n",
    "    env.monitor.start('/tmp/' + env_name + 'QTabRandom')\n",
    "    S = env.reset()\n",
    "    env.render()\n",
    "    for t in range(NO_OF_ITER):\n",
    "        dis_S = get_discrete_state(S)\n",
    "        max_action = max(Q[dis_S], key=Q[dis_S].get)\n",
    "        S, reward, done, _ = env.step([max_action])\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "    env.monitor.close()\n",
    "\n",
    "def evaluation(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    episode_rewards = []\n",
    "    for i in range(NO_OF_EPISODES):\n",
    "        S = env.reset()\n",
    "        episode_reward = 0\n",
    "#         env.render()\n",
    "        for t in range(NO_OF_ITER):\n",
    "            dis_S = get_discrete_state(S)\n",
    "            max_action = max(Q[dis_S], key=Q[dis_S].get)\n",
    "            S, reward, _, _ = env.step([max_action])\n",
    "            if reward == 0:\n",
    "                conti\n",
    "            episode_reward += reward\n",
    "#             env.render()\n",
    "        episode_rewards.append(episode_reward)\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# environments = ['Pendulum-v0','SemisuperPendulumNoise-v0','SemisuperPendulumRandom-v0','SemisuperPendulumDecay-v0']\n",
    "environments = ['SemisuperPendulumRandom-v0']\n",
    "def plot_reward_environments(train, test, legend):\n",
    "    all_legend = []\n",
    "    for reward, l in zip(train,legend):\n",
    "        plt.plot(smooth_graph(reward))\n",
    "        all_legend.append(l+'_train')\n",
    "    for reward, l in zip(test,legend):\n",
    "        plt.plot(smooth_graph(reward))\n",
    "        all_legend.append(l+'_eval')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.legend(all_legend, loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "def test_environments():\n",
    "#     environments = ['Pendulum-v0','SemisuperPendulumNoise-v0','SemisuperPendulumRandom-v0','SemisuperPendulumDecay-v0']\n",
    "#     environments = ['SemisuperPendulumNoise-v0']\n",
    "    all_episodes_train, all_episodes_eval = [], []\n",
    "    all_td_error_train, all_td_error_eval = [], []\n",
    "    for env in environments:\n",
    "        print 'Testing '+ env\n",
    "        episode_rewards_train, td_error_train = qlearning(False, 0.1, 1, env)\n",
    "        save_pickle_dump(\"pickle_dumps/Q\" + filename + \"_\" + env + \".p\", \"ObservedCounts\" + filename + '_' + env + \".p\")\n",
    "#         episode_rewards_eval, td_error_eval = evalution(True, 0, 1, env)\n",
    "#         all_episodes_train.append(copy.deepcopy(episode_rewards_train))\n",
    "#         all_episodes_eval.append(copy.deepcopy(episode_rewards_eval))\n",
    "#         all_td_error_train.append(copy.deepcopy(td_error_train))\n",
    "#         all_td_error_eval.append(copy.deepcopy(td_error_eval))\n",
    "#     plot_reward_environments(all_episodes_train, all_episodes_eval, environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-27 18:52:02,031] Making new env: SemisuperPendulumRandom-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SemisuperPendulumRandom-v0\n",
      "Action Space\n",
      "Box(1,)\n",
      "Observation Space\n",
      "Box(3,)\n",
      "Episode 0\n",
      "Episode 100\n",
      "Episode 200\n",
      "Episode 300\n",
      "Episode 400\n",
      "Episode 500\n",
      "Episode 600\n",
      "Episode 700\n",
      "Episode 800\n",
      "Episode 900\n",
      "Episode 1000\n",
      "Episode 1100\n",
      "Episode 1200\n",
      "Episode 1300\n",
      "Episode 1400\n",
      "Episode 1500\n",
      "Episode 1600\n",
      "Episode 1700\n",
      "Episode 1800\n",
      "Episode 1900\n"
     ]
    }
   ],
   "source": [
    "test_environments()\n",
    "# environments = ['Pendulum-v0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-27 18:55:38,215] Making new env: Pendulum-v0\n",
      "[2016-11-27 18:55:38,223] Creating monitor directory /tmp/Pendulum-v0QTabRandom\n",
      "[2016-11-27 18:55:38,225] Starting new video recorder writing to /tmp/Pendulum-v0QTabRandom/openaigym.video.7.48784.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space\n",
      "Box(1,)\n",
      "Observation Space\n",
      "Box(3,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-27 18:55:44,707] Finished writing results. You can upload them to the scoreboard via gym.upload('/tmp/Pendulum-v0QTabRandom')\n"
     ]
    }
   ],
   "source": [
    "for env in environments:\n",
    "    render(\"pickle_dumps/Q\" + filename + \"_\" + env + \".p\", \"ObservedCounts\" + filename + '_' + env + \".p\", 'Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_Q_values(Q):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    w = 360/ANGLE_TILE_SIZE\n",
    "    h = len(thetadot_bin.keys())\n",
    "    grid = [[0 for x in range(w)] for y in range(h)] \n",
    "    for theta in range(0, 360/ANGLE_TILE_SIZE):\n",
    "        for thetadot in thetadot_bin.keys():\n",
    "            state = (theta, thetadot)\n",
    "            a = max(Q[state], key=Q[state].get)\n",
    "            ax.scatter(theta, thetadot, a, zdir='z')\n",
    "    plt.show()\n",
    "Q, _ = load_pickle_dump(\"Q\" + filename + \"_\" + env + \".p\", \"ObservedCounts\" + filename + '_' + env + \".p\")\n",
    "# print Q\n",
    "plot_Q_values(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
