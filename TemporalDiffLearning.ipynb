{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import defaultdict \n",
    "from __future__ import print_function\n",
    "import random\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import copy\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Actions\n",
    "LEFT = (0, -1)\n",
    "RIGHT = (0, 1)\n",
    "UP = (-1, 0)\n",
    "DOWN = (1, 0)\n",
    "ACTIONS = [LEFT, RIGHT, UP, DOWN]\n",
    "# State Table \n",
    "Q = defaultdict(dict)\n",
    "# Grid Size \n",
    "M = 4\n",
    "N = 12\n",
    "CLIFF_REWARD = -20\n",
    "# Epsilon Value for e-greedy policy \n",
    "EPSILON = 0.2\n",
    "ALPHA = 0.1\n",
    "GAMMA = 1\n",
    "NO_OF_EPISODES = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_valid_state((i, j)):\n",
    "    if i > M or j > N or i < 1 or j < 1:\n",
    "        return False\n",
    "    return True\n",
    "def is_goal_state((i,j)):\n",
    "    if i == M and j == N:\n",
    "        return True\n",
    "    return False\n",
    "def is_cliff_state((i,j)):\n",
    "    if i>= M-1 and i <= M and j >= 2 and j < N:\n",
    "#     if i == M and j >= 2 and j < N:\n",
    "        return True\n",
    "    return False\n",
    "def is_terminal_state((i,j)):\n",
    "    return is_cliff_state((i,j)) or is_goal_state((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_Q():\n",
    "    for i in range(1, M+1):\n",
    "        for j in range(1, N+1):\n",
    "            for action in ACTIONS:\n",
    "                next_state = tuple(map(operator.add, (i,j), action))\n",
    "                if is_valid_state(next_state):\n",
    "                    Q[(i,j)][action] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_action_values_from_state(state):\n",
    "    return Q[state]\n",
    "    action_values = []\n",
    "    actions = []\n",
    "    for action in ACTIONS:\n",
    "        next_state = tuple(map(operator.add, (i,j), action))\n",
    "        if is_valid_state(next_state):\n",
    "            action_values.append(Q[(i,j)][action]) \n",
    "            actions.append(action)\n",
    "    return action_values, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy((i,j)):\n",
    "    state = (i,j)\n",
    "    valid_actions = Q[state].keys()\n",
    "    max_action = max(Q[state], key=Q[state].get)\n",
    "#     print('Max action is %s' % (max_action,))\n",
    "    if random.random() < EPSILON:\n",
    "        random_action = random.choice(valid_actions)\n",
    "#         print('Chose %s' % (random_action,))\n",
    "        return random_action\n",
    "    max_action = max(Q[state], key=Q[state].get)\n",
    "#     print('Chose %s' % (max_action,))\n",
    "    return max_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def take_action(S, A):\n",
    "    next_state = tuple(map(operator.add, S, A))\n",
    "    if is_cliff_state(next_state):\n",
    "        reward = CLIFF_REWARD\n",
    "    else:\n",
    "        reward = -1\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_optimal_policy(Q):\n",
    "    S = (M, 1) # How do u select this ?\n",
    "    optimal_policy = [S]\n",
    "    while not is_terminal_state(S):\n",
    "        max_action = max(Q[S], key=Q[S].get)\n",
    "        next_state, _ = take_action(S, max_action)\n",
    "        print(\"Optimal Policy: Next State %s\"%(next_state,))\n",
    "        optimal_policy.append(next_state)\n",
    "        S = next_state\n",
    "    return optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_optimal_policy(optimal_policy):\n",
    "    for i in range(1, M+1):\n",
    "        for j in range(1, N+1):\n",
    "            state = (i,j)\n",
    "            if (i,j) in optimal_policy:\n",
    "                print('1\\t', end=\"\")\n",
    "            elif is_cliff_state(state):\n",
    "                print('C\\t', end=\"\")\n",
    "            else:\n",
    "                print('0\\t', end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_episode_rewards(all_episode_rewards, legend_values):\n",
    "    for episode_rewards in all_episode_rewards:\n",
    "        plt.plot(episode_rewards)\n",
    "    plt.ylabel('Total Rewards')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.legend(legend_values, loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_episode_steps(episode_steps, legend_values):\n",
    "    for episode_step in episode_steps:\n",
    "        plt.plot(episode_step)\n",
    "    plt.ylabel('Episode steps')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.legend(legend_values, loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth_graph(yvalues):\n",
    "    smooth_values = []\n",
    "    for i in range(len(yvalues) - 10):\n",
    "        l = yvalues[i:i+10]\n",
    "        smooth_values.append(sum(l) / 10)\n",
    "    return smooth_values\n",
    "#     return yvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sarsa():\n",
    "    init_Q()\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    for i in range(NO_OF_EPISODES):\n",
    "        print('Episode Start %s'%(i))\n",
    "        S = (M, 1)\n",
    "        A = epsilon_greedy(S)\n",
    "        total_episode_reward = 0\n",
    "        no_of_steps_per_episode = 0\n",
    "        while not is_terminal_state(S):\n",
    "#             print 'In state %s' % (S,)\n",
    "            S_next, reward = take_action(S, A)\n",
    "            total_episode_reward += reward\n",
    "#             print 'Received Action %s' % (A,)\n",
    "            A_next = epsilon_greedy(S_next)\n",
    "            Q[S][A] = Q[S][A] + ALPHA*(reward + GAMMA*Q[S_next][A_next] - Q[S][A])\n",
    "            S = S_next\n",
    "            A = A_next\n",
    "            no_of_steps_per_episode += 1\n",
    "        episode_rewards.append(total_episode_reward)\n",
    "        episode_steps.append(no_of_steps_per_episode)\n",
    "#         print('Episode End')\n",
    "    optimal_policy = get_optimal_policy(Q)\n",
    "#     for state in Q.keys():\n",
    "#         print(\"%s\"%(state,))\n",
    "#         print(\"%s\"%(Q[state],))\n",
    "#     print(\"%s\"%episode_steps)\n",
    "    return episode_rewards, optimal_policy, episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qlearning():\n",
    "    init_Q()\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    for i in range(NO_OF_EPISODES):\n",
    "        print('Episode Start %s'%(i))\n",
    "        S = (M, 1)\n",
    "        total_episode_reward = 0\n",
    "        no_of_steps_per_episode = 0\n",
    "        while not is_terminal_state(S):\n",
    "            A = epsilon_greedy(S)\n",
    "            S_next, reward = take_action(S, A)\n",
    "            total_episode_reward += reward\n",
    "#             print((Q,))\n",
    "#             print((S_next,))\n",
    "            A_next = max(Q[S_next], key=Q[S_next].get)\n",
    "            Q[S][A] = Q[S][A] + ALPHA*(reward + GAMMA*Q[S_next][A_next] - Q[S][A])\n",
    "            S = S_next\n",
    "            no_of_steps_per_episode += 1\n",
    "        episode_rewards.append(total_episode_reward)\n",
    "        episode_steps.append(no_of_steps_per_episode)\n",
    "    optimal_policy = get_optimal_policy(Q)\n",
    "#     for state in Q.keys():\n",
    "#         print(\"%s\"%(state,))\n",
    "#         print(\"%s\"%(Q[state],))\n",
    "#     print(\"%s\"%episode_steps)\n",
    "    return episode_rewards, optimal_policy, episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "episode_rewards_sarsa, optimal_policy_sarsa, episode_steps_sarsa = sarsa()\n",
    "print('Optimal Policy using Sarsa')\n",
    "print_optimal_policy(optimal_policy_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "episode_rewards_qlearn, optimal_policy_qlearn, episode_steps_qlearning = qlearning()\n",
    "print('Optimal Policy using Q-Learning')\n",
    "print_optimal_policy(optimal_policy_qlearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smooth_rewards_sarsa = smooth_graph(episode_rewards_sarsa)\n",
    "smooth_rewards_qlearn = smooth_graph(episode_rewards_qlearn)\n",
    "smooth_episodes_sarsa = smooth_graph(episode_steps_sarsa)\n",
    "smooth_episodes_qlearn = smooth_graph(episode_steps_qlearning)\n",
    "all_episode_rewards = [smooth_rewards_sarsa, smooth_rewards_qlearn]\n",
    "all_episode_steps = [smooth_episodes_sarsa, smooth_episodes_qlearn]\n",
    "legend_values = [\"Sarsa\", \"Q-Learning\"]\n",
    "plot_episode_rewards(all_episode_rewards, legend_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_episode_steps(all_episode_steps, legend_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
